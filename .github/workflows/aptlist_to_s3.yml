name: Update Apartment List CSV to S3

on:
  schedule:
    - cron: "20 9 19 * *"   # 09:20 UTC on the 19th monthly
  workflow_dispatch: {}

permissions:
  id-token: write
  contents: read

jobs:
  upload:
    runs-on: ubuntu-latest
    concurrency:
      group: aptlist-upload
      cancel-in-progress: false

    steps:
      - uses: actions/checkout@v4

      - name: Assume AWS role (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::607709788146:role/RentSignalsGhOidcUploader
          aws-region: us-east-1

      # --- Headless browser download ---
      - name: Setup Python + Playwright
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: |
          python -m pip install --upgrade pip
          pip install playwright==1.47.0 pandas lxml
          python -m playwright install --with-deps chromium

      - name: Download AptList CSV (headless; script)
        run: |
          set -euo pipefail
          mkdir -p data
          python scripts/download_apartmentlist.py \
            --option "Historic Rent Estimates (Jan 2017 - Present)" \
            --out data/apartmentlist_raw.csv
          head -n 2 data/apartmentlist_raw.csv

      - name: Normalize to long schema
        run: |
          set -euo pipefail
          mkdir -p standardized   # <-- ensure dir exists
          python - <<'PY'
          import re, pandas as pd, pathlib
          src = "data/apartmentlist_raw.csv"
          dst = pathlib.Path("standardized/apartmentlist_long.csv")
          df  = pd.read_csv(src)
          cols = [c.lower().strip() for c in df.columns]
          if 'month' in cols and 'rent_index' in cols:
              rename = {
                'location name':'location_name','location':'location_name',
                'location_type':'location_type',
                'location fips code':'location_fips_code','location_fips_code':'location_fips_code',
                'population':'population','state':'state','county':'county','metro':'metro',
                'rent_index':'rent_index','month':'month'
              }
              df = df.rename(columns={c:rename.get(c.lower().strip(),c) for c in df.columns})
              keep = ['location_name','location_type','location_fips_code','population',
                      'state','county','metro','rent_index','month']
              for k in keep:
                  if k not in df.columns: df[k] = None
              out = df[keep]
          else:
              dfn = df.rename(columns={c:c.lower().strip() for c in df.columns})
              id_candidates = ['location_name','location type','location_type',
                               'location fips code','location_fips_code',
                               'population','state','county','metro']
              id_cols = [c for c in dfn.columns if c in id_candidates] or \
                        [c for c in dfn.columns if c in ['location_name','state','county','metro']]
              date_cols = [c for c in dfn.columns if re.fullmatch(r'\d{4}[-_]\d{2}([-_]\d{2})?', c)]
              if not date_cols: raise SystemExit("No date columns found to melt")
              long = dfn.melt(id_vars=id_cols, value_vars=date_cols,
                              var_name='month', value_name='rent_index')
              long['month'] = pd.to_datetime(long['month'].str.replace('_','-')).dt.date
              out = pd.DataFrame({
                'location_name': long.get('location_name'),
                'location_type': long.get('location_type'),
                'location_fips_code': long.get('location_fips_code'),
                'population': long.get('population'),
                'state': long.get('state'),
                'county': long.get('county'),
                'metro': long.get('metro'),
                'rent_index': long['rent_index'],
                'month': long['month'],
              })
          dst.parent.mkdir(parents=True, exist_ok=True)
          out.to_csv(dst, index=False)
          print("Wrote", dst, "rows:", len(out))
          PY

      # --- Upload to S3 ---
      - name: Upload to S3 (date-stamped prefix)
        env:
          BUCKET: ${{ vars.RENT_SIGNALS_BUCKET }}
        run: |
          set -euo pipefail
          if [[ -z "${BUCKET:-}" ]]; then echo "Set Actions variable RENT_SIGNALS_BUCKET"; exit 1; fi
          TODAY=$(date -u +%F)
          aws s3api put-object --bucket "$BUCKET" --key "aptlist/$TODAY/"
          aws s3 cp standardized/apartmentlist_long.csv "s3://$BUCKET/aptlist/$TODAY/apartmentlist_long.csv"

      - name: Verify S3 upload
        env:
          BUCKET: ${{ vars.RENT_SIGNALS_BUCKET }}
        run: |
          TODAY=$(date -u +%F)
          aws s3 ls "s3://$BUCKET/aptlist/$TODAY/"

      # --- Debug artifacts on failure ---
      - name: Upload debug artifacts (if failed)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: aptlist-debug
          path: |
            data/**
            **/*.html
            **/*.png