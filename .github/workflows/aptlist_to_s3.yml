name: Update Apartment List CSV to S3 (Modular)

on:
  schedule:
    - cron: "20 9 19 * *"   # Monthly on 19th at 09:20 UTC
  workflow_dispatch:
    inputs:
      option_name:
        description: 'Dropdown option to select'
        required: false
        default: 'Historic Rent Estimates (Jan 2017 - Present)'
        type: string

permissions:
  id-token: write
  contents: read

jobs:
  upload:
    runs-on: ubuntu-latest
    concurrency:
      group: aptlist-upload
      cancel-in-progress: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Assume AWS role (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::607709788146:role/RentSignalsGhOidcUploader
          aws-region: us-east-1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright==1.47.0 pandas lxml
          python -m playwright install --with-deps chromium

      - name: Create data directory
        run: mkdir -p data

      - name: Download Apartment List CSV
        env:
          OPTION_NAME: ${{ inputs.option_name || 'Historic Rent Estimates (Jan 2017 - Present)' }}
        run: |
          set -euo pipefail
          python scripts/download_apartmentlist.py \
            --option "$OPTION_NAME" \
            --out data/apartmentlist_raw.csv
          
          echo "Downloaded file preview:"
          head -n 2 data/apartmentlist_raw.csv
          echo "File size: $(wc -l < data/apartmentlist_raw.csv) lines"

      - name: Normalize to long format
        run: |
          set -euo pipefail
          python scripts/normalize_aptlist.py \
            --in data/apartmentlist_raw.csv \
            --out data/apartmentlist_long.csv
          
          echo "Normalized file preview:"
          head -n 2 data/apartmentlist_long.csv
          echo "Final file size: $(wc -l < data/apartmentlist_long.csv) lines"

      - name: Validate output format
        run: |
          set -euo pipefail
          
          # Check that file exists and has content
          if [[ ! -f "data/apartmentlist_long.csv" ]]; then
            echo "Error: Output file not found"
            exit 1
          fi
          
          # Check file has more than just header
          line_count=$(wc -l < data/apartmentlist_long.csv)
          if [[ $line_count -lt 2 ]]; then
            echo "Error: Output file has no data rows"
            exit 1
          fi
          
          # Verify column structure
          expected_cols="location_name,location_type,location_fips_code,population,state,county,metro,rent_index,month"
          actual_header=$(head -n 1 data/apartmentlist_long.csv)
          
          if [[ "$actual_header" != "$expected_cols" ]]; then
            echo "Error: Unexpected column structure"
            echo "Expected: $expected_cols"
            echo "Actual: $actual_header"
            exit 1
          fi
          
          echo "✅ Output validation passed"
          echo "Columns: $actual_header"
          echo "Rows: $((line_count - 1))"

      - name: Upload to S3
        env:
          BUCKET: ${{ vars.RENT_SIGNALS_BUCKET }}
        run: |
          set -euo pipefail
          
          if [[ -z "${BUCKET:-}" ]]; then
            echo "Error: RENT_SIGNALS_BUCKET variable not set"
            exit 1
          fi
          
          TODAY=$(date -u +%F)
          echo "Uploading to S3 bucket: $BUCKET"
          echo "Date prefix: $TODAY"
          
          # Create date prefix
          aws s3api put-object --bucket "$BUCKET" --key "aptlist/$TODAY/"
          
          # Upload the normalized CSV
          aws s3 cp data/apartmentlist_long.csv "s3://$BUCKET/aptlist/$TODAY/apartmentlist_long.csv"
          
          echo "✅ Upload completed"

      - name: Verify S3 upload
        env:
          BUCKET: ${{ vars.RENT_SIGNALS_BUCKET }}
        run: |
          set -euo pipefail
          
          TODAY=$(date -u +%F)
          echo "Verifying upload to s3://$BUCKET/aptlist/$TODAY/"
          
          # List objects in the prefix
          aws s3 ls "s3://$BUCKET/aptlist/$TODAY/"
          
          # Get object metadata
          aws s3api head-object \
            --bucket "$BUCKET" \
            --key "aptlist/$TODAY/apartmentlist_long.csv" \
            --query 'ContentLength' \
            --output text | xargs -I {} echo "Uploaded file size: {} bytes"
          
          echo "✅ Upload verification completed"

      - name: Summary
        if: always()
        env:
          BUCKET: ${{ vars.RENT_SIGNALS_BUCKET }}
          OPTION_NAME: ${{ inputs.option_name || 'Historic Rent Estimates (Jan 2017 - Present)' }}
        run: |
          TODAY=$(date -u +%F)
          echo "=== Apartment List Data Collection Summary ==="
          echo "Selected option: $OPTION_NAME"
          echo "Target S3 location: s3://$BUCKET/aptlist/$TODAY/apartmentlist_long.csv"
          echo "Workflow status: ${{ job.status }}"
          
          if [[ -f "data/apartmentlist_long.csv" ]]; then
            lines=$(wc -l < data/apartmentlist_long.csv)
            echo "Final data rows: $((lines - 1))"
          fi